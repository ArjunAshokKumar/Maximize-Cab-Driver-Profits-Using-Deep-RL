{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 24, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01 \n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = 0.0009\n",
    "        self.epsilon_min = 0.001\n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets    \n",
    "        \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation= 'relu', kernel_initializer= 'he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment    \n",
    "        \n",
    "        possible_actions_index, actions = env.requests(state)\n",
    "        \n",
    "        z = np.random.random()\n",
    "        \n",
    "        if z > self.epsilon:  #Exploitation\n",
    "            state = np.array(env.state_encod_arch2(state)).reshape(1,self.state_size)\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values_possible = np.array([q_values[0][i] for i in possible_actions_index])\n",
    "            q_max_index = np.argmax(q_values_possible)\n",
    "            \n",
    "            return possible_actions_index[q_max_index]\n",
    "        \n",
    "        else:    #Exploration\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, terminal_state):\n",
    "        # Write your code here:\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, terminal_state))\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, terminal_state = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, terminal_state2 = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch2(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch2(next_state)\n",
    "                terminal_state.append(terminal_state2)\n",
    "                \n",
    "                # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)\n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            target_q_net = self.model.predict(update_output)\n",
    "                \n",
    "            # 3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair\n",
    "            for i in range(self.batch_size):\n",
    "    \n",
    "                if terminal_state[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.amax(target_q_net[i])\n",
    "            \n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing necessary variables\n",
    "Episodes = 10000\n",
    "max_run_time = 24*30\n",
    "m = 5 # number of cities, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 7  # number of days, ranges from 0 ... d-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0 \tInitial State:  (2, 14, 6) \tEpisode Reward:  -10 \tTotal Ride Time:  723\n",
      "Episode  5 \tInitial State:  (3, 8, 4) \tEpisode Reward:  -25 \tTotal Ride Time:  723\n",
      "Episode  10 \tInitial State:  (1, 22, 5) \tEpisode Reward:  -186 \tTotal Ride Time:  724\n",
      "Episode  15 \tInitial State:  (0, 23, 6) \tEpisode Reward:  52 \tTotal Ride Time:  720\n",
      "Episode  20 \tInitial State:  (3, 19, 0) \tEpisode Reward:  179 \tTotal Ride Time:  720\n",
      "Episode  25 \tInitial State:  (1, 12, 4) \tEpisode Reward:  -45 \tTotal Ride Time:  721\n",
      "Episode  30 \tInitial State:  (1, 17, 5) \tEpisode Reward:  105 \tTotal Ride Time:  721\n",
      "Episode  35 \tInitial State:  (1, 12, 3) \tEpisode Reward:  55 \tTotal Ride Time:  725\n",
      "Episode  40 \tInitial State:  (2, 20, 5) \tEpisode Reward:  -49 \tTotal Ride Time:  726\n",
      "Episode  45 \tInitial State:  (2, 14, 3) \tEpisode Reward:  101 \tTotal Ride Time:  720\n",
      "Episode  50 \tInitial State:  (1, 5, 1) \tEpisode Reward:  239 \tTotal Ride Time:  721\n",
      "Episode  55 \tInitial State:  (1, 17, 1) \tEpisode Reward:  36 \tTotal Ride Time:  722\n",
      "Episode  60 \tInitial State:  (2, 14, 4) \tEpisode Reward:  -8 \tTotal Ride Time:  725\n",
      "Episode  65 \tInitial State:  (0, 4, 0) \tEpisode Reward:  106 \tTotal Ride Time:  722\n",
      "Episode  70 \tInitial State:  (2, 0, 3) \tEpisode Reward:  55 \tTotal Ride Time:  729\n",
      "Episode  75 \tInitial State:  (4, 5, 2) \tEpisode Reward:  188 \tTotal Ride Time:  721\n",
      "Episode  80 \tInitial State:  (3, 16, 0) \tEpisode Reward:  -448 \tTotal Ride Time:  722\n",
      "Episode  85 \tInitial State:  (1, 10, 0) \tEpisode Reward:  -108 \tTotal Ride Time:  726\n",
      "Episode  90 \tInitial State:  (1, 5, 1) \tEpisode Reward:  -132 \tTotal Ride Time:  724\n",
      "Episode  95 \tInitial State:  (4, 11, 0) \tEpisode Reward:  -122 \tTotal Ride Time:  722\n",
      "Episode  100 \tInitial State:  (4, 10, 3) \tEpisode Reward:  -118 \tTotal Ride Time:  724\n",
      "Episode  105 \tInitial State:  (1, 17, 4) \tEpisode Reward:  116 \tTotal Ride Time:  724\n",
      "Episode  110 \tInitial State:  (2, 22, 3) \tEpisode Reward:  165 \tTotal Ride Time:  727\n",
      "Episode  115 \tInitial State:  (2, 15, 6) \tEpisode Reward:  -159 \tTotal Ride Time:  720\n",
      "Episode  120 \tInitial State:  (3, 18, 3) \tEpisode Reward:  425 \tTotal Ride Time:  734\n",
      "Episode  125 \tInitial State:  (0, 11, 5) \tEpisode Reward:  -223 \tTotal Ride Time:  724\n",
      "Episode  130 \tInitial State:  (3, 0, 3) \tEpisode Reward:  -351 \tTotal Ride Time:  721\n",
      "Episode  135 \tInitial State:  (4, 14, 4) \tEpisode Reward:  -75 \tTotal Ride Time:  731\n",
      "Episode  140 \tInitial State:  (3, 1, 5) \tEpisode Reward:  -103 \tTotal Ride Time:  722\n",
      "Episode  145 \tInitial State:  (0, 3, 0) \tEpisode Reward:  -267 \tTotal Ride Time:  720\n",
      "Episode  150 \tInitial State:  (3, 13, 1) \tEpisode Reward:  15 \tTotal Ride Time:  720\n",
      "Episode  155 \tInitial State:  (4, 20, 4) \tEpisode Reward:  56 \tTotal Ride Time:  723\n",
      "Episode  160 \tInitial State:  (2, 14, 0) \tEpisode Reward:  -31 \tTotal Ride Time:  729\n",
      "Episode  165 \tInitial State:  (3, 10, 6) \tEpisode Reward:  -112 \tTotal Ride Time:  726\n",
      "Episode  170 \tInitial State:  (0, 12, 1) \tEpisode Reward:  -139 \tTotal Ride Time:  725\n",
      "Episode  175 \tInitial State:  (3, 18, 2) \tEpisode Reward:  -89 \tTotal Ride Time:  721\n",
      "Episode  180 \tInitial State:  (3, 1, 5) \tEpisode Reward:  141 \tTotal Ride Time:  724\n",
      "Episode  185 \tInitial State:  (2, 12, 1) \tEpisode Reward:  -382 \tTotal Ride Time:  720\n",
      "Episode  190 \tInitial State:  (3, 6, 5) \tEpisode Reward:  255 \tTotal Ride Time:  722\n",
      "Episode  195 \tInitial State:  (4, 10, 4) \tEpisode Reward:  -238 \tTotal Ride Time:  721\n",
      "Episode  200 \tInitial State:  (2, 5, 2) \tEpisode Reward:  -82 \tTotal Ride Time:  724\n",
      "Episode  205 \tInitial State:  (0, 2, 1) \tEpisode Reward:  165 \tTotal Ride Time:  723\n",
      "Episode  210 \tInitial State:  (0, 22, 4) \tEpisode Reward:  -172 \tTotal Ride Time:  726\n",
      "Episode  215 \tInitial State:  (0, 19, 5) \tEpisode Reward:  -10 \tTotal Ride Time:  720\n",
      "Episode  220 \tInitial State:  (2, 13, 5) \tEpisode Reward:  -139 \tTotal Ride Time:  723\n",
      "Episode  225 \tInitial State:  (2, 19, 4) \tEpisode Reward:  -254 \tTotal Ride Time:  725\n",
      "Episode  230 \tInitial State:  (3, 16, 5) \tEpisode Reward:  196 \tTotal Ride Time:  730\n",
      "Episode  235 \tInitial State:  (3, 4, 0) \tEpisode Reward:  82 \tTotal Ride Time:  725\n",
      "Episode  240 \tInitial State:  (1, 1, 2) \tEpisode Reward:  -174 \tTotal Ride Time:  720\n",
      "Episode  245 \tInitial State:  (4, 13, 1) \tEpisode Reward:  317 \tTotal Ride Time:  730\n",
      "Episode  250 \tInitial State:  (0, 21, 5) \tEpisode Reward:  -17 \tTotal Ride Time:  731\n",
      "Episode  255 \tInitial State:  (3, 3, 0) \tEpisode Reward:  -21 \tTotal Ride Time:  728\n",
      "Episode  260 \tInitial State:  (1, 2, 5) \tEpisode Reward:  -30 \tTotal Ride Time:  721\n",
      "Episode  265 \tInitial State:  (4, 2, 6) \tEpisode Reward:  41 \tTotal Ride Time:  726\n",
      "Episode  270 \tInitial State:  (4, 15, 5) \tEpisode Reward:  230 \tTotal Ride Time:  725\n",
      "Episode  275 \tInitial State:  (0, 3, 1) \tEpisode Reward:  118 \tTotal Ride Time:  723\n",
      "Episode  280 \tInitial State:  (4, 11, 5) \tEpisode Reward:  -336 \tTotal Ride Time:  720\n",
      "Episode  285 \tInitial State:  (3, 21, 5) \tEpisode Reward:  8 \tTotal Ride Time:  725\n",
      "Episode  290 \tInitial State:  (1, 2, 0) \tEpisode Reward:  -458 \tTotal Ride Time:  726\n",
      "Episode  295 \tInitial State:  (0, 7, 1) \tEpisode Reward:  -375 \tTotal Ride Time:  726\n",
      "Episode  300 \tInitial State:  (3, 14, 1) \tEpisode Reward:  -110 \tTotal Ride Time:  722\n",
      "Episode  305 \tInitial State:  (3, 7, 4) \tEpisode Reward:  -1 \tTotal Ride Time:  729\n",
      "Episode  310 \tInitial State:  (2, 1, 2) \tEpisode Reward:  53 \tTotal Ride Time:  720\n",
      "Episode  315 \tInitial State:  (0, 20, 1) \tEpisode Reward:  167 \tTotal Ride Time:  726\n",
      "Episode  320 \tInitial State:  (1, 9, 1) \tEpisode Reward:  212 \tTotal Ride Time:  724\n",
      "Episode  325 \tInitial State:  (4, 11, 1) \tEpisode Reward:  -129 \tTotal Ride Time:  731\n",
      "Episode  330 \tInitial State:  (2, 5, 5) \tEpisode Reward:  44 \tTotal Ride Time:  721\n",
      "Episode  335 \tInitial State:  (2, 4, 2) \tEpisode Reward:  -169 \tTotal Ride Time:  728\n",
      "Episode  340 \tInitial State:  (0, 6, 5) \tEpisode Reward:  86 \tTotal Ride Time:  725\n",
      "Episode  345 \tInitial State:  (1, 14, 6) \tEpisode Reward:  196 \tTotal Ride Time:  725\n",
      "Episode  350 \tInitial State:  (1, 7, 4) \tEpisode Reward:  352 \tTotal Ride Time:  721\n",
      "Episode  355 \tInitial State:  (2, 6, 0) \tEpisode Reward:  97 \tTotal Ride Time:  723\n",
      "Episode  360 \tInitial State:  (1, 22, 5) \tEpisode Reward:  75 \tTotal Ride Time:  722\n",
      "Episode  365 \tInitial State:  (3, 3, 4) \tEpisode Reward:  447 \tTotal Ride Time:  725\n",
      "Episode  370 \tInitial State:  (1, 12, 1) \tEpisode Reward:  52 \tTotal Ride Time:  722\n",
      "Episode  375 \tInitial State:  (3, 3, 5) \tEpisode Reward:  -15 \tTotal Ride Time:  724\n",
      "Episode  380 \tInitial State:  (3, 20, 2) \tEpisode Reward:  134 \tTotal Ride Time:  720\n",
      "Episode  385 \tInitial State:  (2, 19, 6) \tEpisode Reward:  -56 \tTotal Ride Time:  724\n",
      "Episode  390 \tInitial State:  (0, 2, 3) \tEpisode Reward:  37 \tTotal Ride Time:  729\n",
      "Episode  395 \tInitial State:  (3, 23, 5) \tEpisode Reward:  -82 \tTotal Ride Time:  720\n",
      "Episode  400 \tInitial State:  (0, 20, 1) \tEpisode Reward:  178 \tTotal Ride Time:  723\n",
      "Episode  405 \tInitial State:  (0, 21, 4) \tEpisode Reward:  -206 \tTotal Ride Time:  720\n",
      "Episode  410 \tInitial State:  (0, 15, 1) \tEpisode Reward:  -65 \tTotal Ride Time:  728\n",
      "Episode  415 \tInitial State:  (3, 14, 5) \tEpisode Reward:  -383 \tTotal Ride Time:  722\n",
      "Episode  420 \tInitial State:  (3, 21, 0) \tEpisode Reward:  57 \tTotal Ride Time:  722\n",
      "Episode  425 \tInitial State:  (0, 21, 3) \tEpisode Reward:  32 \tTotal Ride Time:  727\n",
      "Episode  430 \tInitial State:  (3, 16, 0) \tEpisode Reward:  5 \tTotal Ride Time:  727\n",
      "Episode  435 \tInitial State:  (4, 13, 6) \tEpisode Reward:  123 \tTotal Ride Time:  726\n",
      "Episode  440 \tInitial State:  (4, 23, 5) \tEpisode Reward:  5 \tTotal Ride Time:  729\n",
      "Episode  445 \tInitial State:  (1, 21, 0) \tEpisode Reward:  59 \tTotal Ride Time:  732\n",
      "Episode  450 \tInitial State:  (4, 1, 4) \tEpisode Reward:  -71 \tTotal Ride Time:  727\n",
      "Episode  455 \tInitial State:  (3, 5, 5) \tEpisode Reward:  5 \tTotal Ride Time:  723\n",
      "Episode  460 \tInitial State:  (1, 20, 1) \tEpisode Reward:  173 \tTotal Ride Time:  727\n",
      "Episode  465 \tInitial State:  (2, 12, 4) \tEpisode Reward:  -17 \tTotal Ride Time:  726\n",
      "Episode  470 \tInitial State:  (2, 22, 5) \tEpisode Reward:  255 \tTotal Ride Time:  725\n",
      "Episode  475 \tInitial State:  (4, 6, 4) \tEpisode Reward:  137 \tTotal Ride Time:  721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  480 \tInitial State:  (1, 20, 2) \tEpisode Reward:  5 \tTotal Ride Time:  731\n",
      "Episode  485 \tInitial State:  (3, 4, 6) \tEpisode Reward:  -289 \tTotal Ride Time:  723\n",
      "Episode  490 \tInitial State:  (2, 8, 1) \tEpisode Reward:  41 \tTotal Ride Time:  722\n",
      "Episode  495 \tInitial State:  (2, 20, 2) \tEpisode Reward:  -237 \tTotal Ride Time:  728\n",
      "Episode  500 \tInitial State:  (4, 19, 2) \tEpisode Reward:  6 \tTotal Ride Time:  720\n",
      "Episode  505 \tInitial State:  (1, 7, 1) \tEpisode Reward:  171 \tTotal Ride Time:  722\n",
      "Episode  510 \tInitial State:  (1, 3, 2) \tEpisode Reward:  146 \tTotal Ride Time:  726\n",
      "Episode  515 \tInitial State:  (3, 23, 0) \tEpisode Reward:  -268 \tTotal Ride Time:  723\n",
      "Episode  520 \tInitial State:  (2, 11, 2) \tEpisode Reward:  -210 \tTotal Ride Time:  721\n",
      "Episode  525 \tInitial State:  (0, 2, 6) \tEpisode Reward:  -75 \tTotal Ride Time:  726\n",
      "Episode  530 \tInitial State:  (1, 4, 1) \tEpisode Reward:  -62 \tTotal Ride Time:  724\n",
      "Episode  535 \tInitial State:  (2, 4, 4) \tEpisode Reward:  -170 \tTotal Ride Time:  720\n",
      "Episode  540 \tInitial State:  (1, 6, 3) \tEpisode Reward:  177 \tTotal Ride Time:  726\n",
      "Episode  545 \tInitial State:  (1, 14, 3) \tEpisode Reward:  59 \tTotal Ride Time:  734\n",
      "Episode  550 \tInitial State:  (4, 14, 4) \tEpisode Reward:  -98 \tTotal Ride Time:  724\n",
      "Episode  555 \tInitial State:  (4, 1, 1) \tEpisode Reward:  -80 \tTotal Ride Time:  727\n",
      "Episode  560 \tInitial State:  (0, 4, 5) \tEpisode Reward:  -172 \tTotal Ride Time:  728\n",
      "Episode  565 \tInitial State:  (1, 16, 1) \tEpisode Reward:  167 \tTotal Ride Time:  727\n",
      "Episode  570 \tInitial State:  (2, 21, 3) \tEpisode Reward:  -4 \tTotal Ride Time:  729\n",
      "Episode  575 \tInitial State:  (1, 13, 4) \tEpisode Reward:  50 \tTotal Ride Time:  729\n",
      "Episode  580 \tInitial State:  (1, 6, 6) \tEpisode Reward:  -142 \tTotal Ride Time:  720\n",
      "Episode  585 \tInitial State:  (0, 13, 2) \tEpisode Reward:  263 \tTotal Ride Time:  722\n",
      "Episode  590 \tInitial State:  (1, 15, 6) \tEpisode Reward:  -282 \tTotal Ride Time:  721\n",
      "Episode  595 \tInitial State:  (2, 10, 6) \tEpisode Reward:  -103 \tTotal Ride Time:  723\n",
      "Episode  600 \tInitial State:  (1, 0, 0) \tEpisode Reward:  152 \tTotal Ride Time:  723\n",
      "Episode  605 \tInitial State:  (3, 0, 1) \tEpisode Reward:  -148 \tTotal Ride Time:  723\n",
      "Episode  610 \tInitial State:  (3, 17, 2) \tEpisode Reward:  -93 \tTotal Ride Time:  725\n",
      "Episode  615 \tInitial State:  (0, 2, 5) \tEpisode Reward:  30 \tTotal Ride Time:  727\n",
      "Episode  620 \tInitial State:  (1, 13, 1) \tEpisode Reward:  136 \tTotal Ride Time:  722\n",
      "Episode  625 \tInitial State:  (3, 18, 2) \tEpisode Reward:  -164 \tTotal Ride Time:  730\n",
      "Episode  630 \tInitial State:  (4, 17, 4) \tEpisode Reward:  -137 \tTotal Ride Time:  723\n",
      "Episode  635 \tInitial State:  (1, 7, 0) \tEpisode Reward:  51 \tTotal Ride Time:  723\n",
      "Episode  640 \tInitial State:  (4, 5, 5) \tEpisode Reward:  77 \tTotal Ride Time:  727\n",
      "Episode  645 \tInitial State:  (2, 4, 5) \tEpisode Reward:  113 \tTotal Ride Time:  729\n",
      "Episode  650 \tInitial State:  (4, 1, 2) \tEpisode Reward:  16 \tTotal Ride Time:  721\n",
      "Episode  655 \tInitial State:  (1, 21, 5) \tEpisode Reward:  142 \tTotal Ride Time:  728\n",
      "Episode  660 \tInitial State:  (4, 10, 3) \tEpisode Reward:  92 \tTotal Ride Time:  722\n",
      "Episode  665 \tInitial State:  (1, 23, 2) \tEpisode Reward:  -76 \tTotal Ride Time:  727\n",
      "Episode  670 \tInitial State:  (0, 23, 3) \tEpisode Reward:  -137 \tTotal Ride Time:  720\n",
      "Episode  675 \tInitial State:  (3, 10, 3) \tEpisode Reward:  -161 \tTotal Ride Time:  720\n",
      "Episode  680 \tInitial State:  (3, 10, 5) \tEpisode Reward:  -13 \tTotal Ride Time:  727\n",
      "Episode  685 \tInitial State:  (2, 0, 6) \tEpisode Reward:  -80 \tTotal Ride Time:  725\n",
      "Episode  690 \tInitial State:  (4, 21, 6) \tEpisode Reward:  -46 \tTotal Ride Time:  721\n",
      "Episode  695 \tInitial State:  (4, 16, 4) \tEpisode Reward:  -7 \tTotal Ride Time:  722\n",
      "Episode  700 \tInitial State:  (3, 6, 0) \tEpisode Reward:  -4 \tTotal Ride Time:  724\n",
      "Episode  705 \tInitial State:  (4, 10, 0) \tEpisode Reward:  -182 \tTotal Ride Time:  729\n",
      "Episode  710 \tInitial State:  (0, 21, 1) \tEpisode Reward:  14 \tTotal Ride Time:  723\n",
      "Episode  715 \tInitial State:  (3, 23, 5) \tEpisode Reward:  114 \tTotal Ride Time:  721\n",
      "Episode  720 \tInitial State:  (4, 5, 1) \tEpisode Reward:  -44 \tTotal Ride Time:  726\n",
      "Episode  725 \tInitial State:  (1, 17, 2) \tEpisode Reward:  80 \tTotal Ride Time:  724\n",
      "Episode  730 \tInitial State:  (2, 17, 3) \tEpisode Reward:  -141 \tTotal Ride Time:  722\n",
      "Episode  735 \tInitial State:  (4, 7, 2) \tEpisode Reward:  3 \tTotal Ride Time:  725\n",
      "Episode  740 \tInitial State:  (2, 21, 5) \tEpisode Reward:  55 \tTotal Ride Time:  726\n",
      "Episode  745 \tInitial State:  (4, 23, 3) \tEpisode Reward:  52 \tTotal Ride Time:  720\n",
      "Episode  750 \tInitial State:  (3, 11, 4) \tEpisode Reward:  -17 \tTotal Ride Time:  720\n",
      "Episode  755 \tInitial State:  (2, 14, 0) \tEpisode Reward:  -160 \tTotal Ride Time:  720\n",
      "Episode  760 \tInitial State:  (0, 22, 1) \tEpisode Reward:  179 \tTotal Ride Time:  722\n",
      "Episode  765 \tInitial State:  (3, 4, 4) \tEpisode Reward:  -110 \tTotal Ride Time:  721\n",
      "Episode  770 \tInitial State:  (0, 4, 2) \tEpisode Reward:  81 \tTotal Ride Time:  720\n",
      "Episode  775 \tInitial State:  (4, 1, 5) \tEpisode Reward:  214 \tTotal Ride Time:  720\n",
      "Episode  780 \tInitial State:  (0, 14, 6) \tEpisode Reward:  -129 \tTotal Ride Time:  726\n",
      "Episode  785 \tInitial State:  (2, 9, 6) \tEpisode Reward:  -82 \tTotal Ride Time:  727\n",
      "Episode  790 \tInitial State:  (2, 0, 3) \tEpisode Reward:  -211 \tTotal Ride Time:  725\n",
      "Episode  795 \tInitial State:  (4, 20, 1) \tEpisode Reward:  -111 \tTotal Ride Time:  729\n",
      "Episode  800 \tInitial State:  (0, 11, 3) \tEpisode Reward:  -24 \tTotal Ride Time:  721\n",
      "Episode  805 \tInitial State:  (4, 14, 5) \tEpisode Reward:  144 \tTotal Ride Time:  725\n",
      "Episode  810 \tInitial State:  (3, 5, 0) \tEpisode Reward:  233 \tTotal Ride Time:  724\n",
      "Episode  815 \tInitial State:  (1, 5, 6) \tEpisode Reward:  249 \tTotal Ride Time:  733\n",
      "Episode  820 \tInitial State:  (4, 0, 1) \tEpisode Reward:  -348 \tTotal Ride Time:  722\n",
      "Episode  825 \tInitial State:  (0, 9, 0) \tEpisode Reward:  147 \tTotal Ride Time:  725\n",
      "Episode  830 \tInitial State:  (3, 9, 1) \tEpisode Reward:  104 \tTotal Ride Time:  726\n",
      "Episode  835 \tInitial State:  (2, 4, 3) \tEpisode Reward:  187 \tTotal Ride Time:  727\n",
      "Episode  840 \tInitial State:  (1, 5, 4) \tEpisode Reward:  -355 \tTotal Ride Time:  725\n",
      "Episode  845 \tInitial State:  (1, 19, 4) \tEpisode Reward:  -431 \tTotal Ride Time:  725\n",
      "Episode  850 \tInitial State:  (0, 2, 4) \tEpisode Reward:  46 \tTotal Ride Time:  727\n",
      "Episode  855 \tInitial State:  (2, 8, 4) \tEpisode Reward:  140 \tTotal Ride Time:  722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56304/4245527841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# 4. Train the model by calling function dqn_agent.train_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# 5. Keep a track of rewards, Q-values, loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mreward_this_episode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_56304/3550152158.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# 2. Get the target for the Q-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtarget_q_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# 3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/msmlai/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time() #to calculate the time of each episode\n",
    "#rewards_tracked = []\n",
    "rewards_per_episode =[]\n",
    "episodes = []\n",
    "\n",
    "#Getting info from CabDriver to invoke DQNAgent object\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "    terminal_state = False\n",
    "    reward_this_episode = 0\n",
    "    \n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    init_state = state\n",
    "    total_time = 0 #time drive time for this episode\n",
    "    \n",
    "    while not (terminal_state):\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action_index = dqn_agent.get_action(state)\n",
    "        action = env.action_space[action_index]\n",
    "        \n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = env.reward_func(state, action, Time_matrix)\n",
    "        next_state = env.next_state_func(state, action, Time_matrix)\n",
    "        \n",
    "        #calculate the time taken as the max time is 24*30 hrs.\n",
    "        time_for_step = env.calc_total_time(state, action, Time_matrix)\n",
    "        total_time += time_for_step\n",
    "        if total_time < max_run_time:\n",
    "            # 3. Append the experience to the memory\n",
    "            dqn_agent.append_sample(state, action_index, reward, next_state, terminal_state)\n",
    "            # 4. Train the model by calling function dqn_agent.train_model\n",
    "            dqn_agent.train_model()\n",
    "            # 5. Keep a track of rewards, Q-values, loss\n",
    "            reward_this_episode += reward\n",
    "            state = next_state\n",
    "        else:\n",
    "            terminal_state = True\n",
    "        \n",
    "    rewards_per_episode.append(reward_this_episode)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    #set the epsilon value\n",
    "    dqn_agent.epsilon = dqn_agent.epsilon_min + (dqn_agent.epsilon_max - dqn_agent.epsilon_min) * np.exp(dqn_agent.epsilon_decay * episode)\n",
    "\n",
    "    if episode % 5 == 0:\n",
    "        print('Episode ',episode, \n",
    "              '\\tInitial State: ', init_state, \n",
    "              '\\tEpisode Reward: ',reward_this_episode, \n",
    "              '\\tTotal Ride Time: ', total_time)\n",
    "\n",
    "    if episode % 2000 == 0:\n",
    "        dqn_agent.save('CabDriver_DQN_Model.h5')\n",
    "\n",
    "end_time = time.time()\n",
    "print('Elapsed time: ', (start_time-end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = CabDriver()\n",
    "action_space, state_space, state = temp.reset()\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "print(action_size, state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m+t+d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = np.arange(0,10000)\n",
    "# epsilon = []\n",
    "# for i in range(0,10000):\n",
    "#     epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(time, epsilon)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
